export default [
	{
		timeslot: 'Typedesign with Karlo',
		title: 'Markus Mau (Hochschule Mainz)',
		description: `Generative AI models like Dalle, Midjourney, or Stable-Diffusion are increasingly making their
        mark in our design practice. In this workshop, we want to apply such innovative AI tools to
        the field of typography. Does the technology offer a new visual language? How does the
        design process change, and what opportunities does this offer for creative practice?
        Together, we will explore these and many other questions in the workshop. You will work
        hands-on with "intelligent tools" and develop your own typeface in collaboration with the
        Karlo AI model.`
	},
	{
		timeslot: 'Making (Non)Sense – Prototyping AI based Interactive Systems with Sensors',
		title: 'Rahel Flechtner (HfG Schwäbisch-Gmünd) & Jakob Kilian (KISD)',
		description: `In this workshop, we will explore the possibilities of sensor data and gain a deeper understanding
        of how it can be used to create interactive AI based systems.
        In the first part of the workshop, we'll be acting out AI systems in an interactive and playful
        manner, allowing you to gain insights into how these systems extract valuable information from
        data. This will be followed by a more practical part in which you'll learn how to prototype and
        train your own AI-based interactive systems. But this isn't just any interactive system! Inspired by
        the iconic Rube Goldberg machine, we'll be experimenting with a range of inputs and outputs to
        create playful interactions through the concatenation of everyday objects, sensors, and
        microcontrollers.`
	},
	{
		timeslot: 'PLAYTRACING',
		title: 'Simon Maris (Hochschule Trier)',
		description: `The process of visualising products, interiors, buildings and urban situations is
        about to change radically. Traditionally, render engines have been concerned
        with simulating the physical behaviour of light by bouncing rays of light through
        a virtual model. This requires creating accurate 3D digital models, texturing them,
        adding light sources and more. Now, machine learning systems are introducing
        a very different workflow. While models such as *stable diffusion* alone do not offer
        much control over the outcome, more recently plug-ins and other models with
        various input controls are entering the scene.
        We will explore these new workflows through playful experimentation,
        effortlessly blending and switching between the physical and digital worlds,
        combining multiple software tools and traditional modelling. It is all about
        discovering and enabling more spontaneity, and ultimately allowing more time
        to focus on composition, lighting, mood and attention to detail. Let's discover a
        new research tool and communicate our ideas, bring your own models, software,
        image making tools and visions!`
	},
	{
		timeslot: 'Designing Prompts',
		title: 'Benedikt Groß (HfG Schwäbisch-Gmünd)',
		description: ''
	},
	{
		timeslot: 'tbd',
		title: 'Aeneas Stankowski (HfG Schwäbisch-Gmünd)',
		description: ''
	}
];
